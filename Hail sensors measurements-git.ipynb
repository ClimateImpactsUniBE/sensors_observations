{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d97abe-299f-4f7c-8385-521f4bc389c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import scipy.stats as stats\n",
    "from scipy.optimize import curve_fit\n",
    "import datetime\n",
    "from datetime import timezone, timedelta\n",
    "from dateutil.parser import parse\n",
    "import numpy as np\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "import xarray as xr\n",
    "\n",
    "import cartopy.feature as cfeature\n",
    "import cartopy.crs as ccrs\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "import cartopy.io.shapereader as shpreader\n",
    "import matplotlib\n",
    "#matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib import colors\n",
    "import matplotlib.lines as mlines\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib.ticker import (MultipleLocator, FormatStrFormatter,\n",
    "                               AutoMinorLocator)\n",
    "from matplotlib.colors import LinearSegmentedColormap,ListedColormap\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.patheffects as path_effects\n",
    "#plt.rc('mathtext', fontset=\"cm\")\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from pyproj import Transformer\n",
    "from shapely.geometry import Polygon\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import explained_variance_score, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb9270c-c9a3-43e1-8cd1-1d79c7d9f45d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3afb0e-eaf8-44d1-aa69-ffb7a74ea3c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# User inputs\n",
    "###############################################################################\n",
    "REPORT_FOLDER = 'C:/.../reports/' # Folder of hail reports in CSV format\n",
    "REPORT_FOLDER_4km = 'C:/.../reports_4km/' # Folder of hail reports in CSV format\n",
    "T0 = '20150502' # Start of time range #20180709\n",
    "T1 = '20220903' # End of time range #20210802\n",
    "DPI = 300 # DPI for the plots\n",
    "OUTPUT_FOLDER = 'C:/.../plots' # output folder for plots, a subfolder will be created for plots\n",
    "EXCLUDE_SENSORS = ['SchulhausHASLE']\n",
    "EXCLUDE_DATES = ['2020-06-15','2020-06-16','2020-06-17','-06-18']\n",
    "EXCLUDE_SENSORS_2 = ['RsiLUGANO','SportivoRIVERA','IdaMEDEGLIA']\n",
    "EXCLUDE_DATES_2 = ['2019-03-11','2019-03-17','2019-03-18','2020-06-18']\n",
    "EXCLUDE_SENSORS_3 = ['SportivoRIVERA']\n",
    "EXCLUDE_DATES_3 = ['2019-09-05']\n",
    "EXCLUDE_SENSORS_4 = ['UniBE']\n",
    "EXCLUDE_DATES_4 = ['2021-06-29']\n",
    "#EVENT_MIN_TIME = 600 #Minimum time separating two hail events in seconds\n",
    "MIN_CZC = 35 #Minimum max reflectivity at sensor location to filter out non hail impacts\n",
    "SENSOR_SURFACE = 0.25**2 * np.pi # sensor surface in m2\n",
    "SENSOR_SURFACE_KM = SENSOR_SURFACE * 10**-6\n",
    "folder_dt_analysis = foldername + 'sensitivity_analysis/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1f819a-2099-4107-ab63-b56b55b5c362",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ceil_dt(dt, delta = 5):\n",
    "    delta = datetime.timedelta(minutes=delta)\n",
    "    return dt + (datetime.datetime.min - dt) % delta\n",
    "\n",
    "# Create output folder\n",
    "foldername = OUTPUT_FOLDER + '/plots_hail_{:s}_{:s}/'.format(T0,T1)\n",
    "if not os.path.exists(foldername):\n",
    "    os.makedirs(foldername)\n",
    "    \n",
    "# Convert to datetime for comparison\n",
    "T0 = datetime.datetime.strptime(T0,'%Y%m%d')\n",
    "T1 = datetime.datetime.strptime(T1,'%Y%m%d')\n",
    "\n",
    "def concate_report(folder):\n",
    "    # Read all reports in time range\n",
    "    all_files = sorted(glob.glob(folder + '*.txt'))\n",
    "    reports_data = list()\n",
    "    for f in all_files:\n",
    "        date = datetime.datetime.strptime(os.path.basename(f).split('_')[-1].split('.')[0], '%Y%m%d')\n",
    "        if date >= T0 and date <= T1:\n",
    "            reports_data.append(pd.read_csv(f))\n",
    "    reports_data = pd.concat(reports_data)\n",
    "    return reports_data\n",
    "\n",
    "reports_4km = concate_report(REPORT_FOLDER_4km)\n",
    "reports_10km = concate_report(REPORT_FOLDER)\n",
    "reports_10km = reports_10km[['sensorName','Time','maxpoh_rad10','maxmeshs_rad10','maxczc_rad10']]\n",
    "\n",
    "reports_data = pd.merge(reports_4km, reports_10km, how='inner', on=['sensorName','Time'])\n",
    "\n",
    "# Add day and closest radar timestamp to df\n",
    "reports_data['days'] = [a[0:10] for a in reports_data['Time'].values]\n",
    "reports_data['months'] = [a[0:7] for a in reports_data['Time'].values]\n",
    "reports_data['years'] = [a[0:4] for a in reports_data['Time'].values]\n",
    "reports_data['rad_timestamp'] = [ceil_dt(parse(a), 5) for a in reports_data['Time'].values]\n",
    "reports_data['digits'] = reports_data['digits'].astype(np.int64)\n",
    "\n",
    "reports_data = reports_data[~((reports_data['sensorName'].isin(EXCLUDE_SENSORS)) &\n",
    "                              (reports_data['days'].isin(EXCLUDE_DATES)))]\n",
    "\n",
    "reports_data = reports_data[~((reports_data['sensorName'].isin(EXCLUDE_SENSORS_2)) &\n",
    "                              (reports_data['days'].isin(EXCLUDE_DATES_2)))]\n",
    "\n",
    "reports_data = reports_data[~((reports_data['sensorName'].isin(EXCLUDE_SENSORS_3)) &\n",
    "                              (reports_data['days'].isin(EXCLUDE_DATES_3)))]\n",
    "\n",
    "reports_data = reports_data[~((reports_data['sensorName'].isin(EXCLUDE_SENSORS_4)) &\n",
    "                              (reports_data['days'].isin(EXCLUDE_DATES_4)))]\n",
    "\n",
    "path_install_date = 'C:/.../Hail Sensors/'\n",
    "file_install_date = 'sensors_install_dates.csv'\n",
    "\n",
    "install_data = pd.read_csv(path_install_date + file_install_date, sep=';',dtype={'sensorName':str, 'installDate': str})\n",
    "reports_data = pd.merge(reports_data, install_data, how=\"inner\", on = 'sensorName')\n",
    "reports_data['installDate'] = pd.to_datetime(reports_data['installDate'])\n",
    "\n",
    "#inProj = 'epsg:2056' # CH1903+ / LV95, see https://epsg.io/2056\n",
    "inProj = 'epsg:21781' # CH1903 / LV03, see https://epsg.io/21781\n",
    "outProj = 'epsg:4326' # WGS84, see https://epsg.io/4326\n",
    "\n",
    "transformer = Transformer.from_crs(inProj, outProj)\n",
    "coords = transformer.transform(reports_data['CHY'], reports_data['CHX'])\n",
    "\n",
    "reports_data['Lat'] = coords[0].tolist()\n",
    "reports_data['Lon'] = coords[1].tolist()\n",
    "\n",
    "conditions = [(reports_data['CHX'] < 140000),((reports_data['CHX'] > 140000) & (reports_data['CHX'] < 220000)),\n",
    "    (reports_data['CHX'] > 220000)]\n",
    "val = ['Ticino', 'Napf', 'Jura']\n",
    "reports_data['region'] = np.select(conditions, val)\n",
    "\n",
    "\n",
    "sensors_list = reports_data.groupby('sensorName').agg({'Lon': 'first', 'Lat': 'first', 'CHX': 'first', 'CHY': 'first',\n",
    "                                                      'installDate': 'first', 'region': 'first'}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f28fbf-5d33-4d4c-94fa-997a1e800b6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_sensors_filtered(reports_data, filter_field, field_value):\n",
    "    # Filtered data\n",
    "    sensors_filtered = reports_data[(reports_data['status'] == 0) & (reports_data['diameter'] > 0)\n",
    "                                    & (reports_data['kin_energy'] > 0) & (reports_data['digits'] > 50)\n",
    "                                    & (reports_data['calib_factor'] != 0) & (reports_data['calib_factor'].notnull())\n",
    "                                   & (reports_data[filter_field] >= field_value)]\n",
    "\n",
    "    # Remove duplicates (same sensor and same Time)\n",
    "    sensors_filtered = sensors_filtered.drop_duplicates(subset=['sensorName', 'Time'], keep='first')\n",
    "\n",
    "    temp = sensors_filtered.groupby(['sensorName','days'])\n",
    "    sensors_filtered = sensors_filtered.set_index(['sensorName', 'days'])\n",
    "    sensors_filtered['daily_count_by_sensor'] = temp.size()\n",
    "    sensors_filtered['day_sensor_ID'] = sensors_filtered.groupby(['sensorName','days']).ngroup()\n",
    "    sensors_filtered = sensors_filtered.reset_index()\n",
    "    sensors_filtered['Day_D_max'] = sensors_filtered.groupby(['day_sensor_ID'])['diameter'].transform('max')\n",
    "\n",
    "    sensors_filtered['diameter_est'] = np.power(sensors_filtered['kin_energy']/0.0462285,1/4)*10\n",
    "    sensors_filtered['vt_est'] = 13.77 * np.power(sensors_filtered['diameter_est']/10,0.5)\n",
    "\n",
    "    sensors_filtered['m_est'] = 2 * sensors_filtered['kin_energy']/(sensors_filtered['vt_est']**2)\n",
    "\n",
    "    sensors_filtered['d_H20'] = np.where(sensors_filtered['diameter_est']<22.1,\n",
    "                                                    np.power(sensors_filtered['kin_energy']/0.0107,1/4.59)*10,\n",
    "                                                    np.power(sensors_filtered['kin_energy']/0.0243,1/3.55)*10)\n",
    "\n",
    "    sensors_filtered['m_H20'] = 0.372 * np.power(sensors_filtered['d_H20']/10,2.69)\n",
    "\n",
    "    sensors_filtered['area_H20'] = 0.62 * np.power(sensors_filtered['d_H20']/10,1.96)\n",
    "    sensors_filtered['area_est'] = np.pi/4 * np.power(sensors_filtered['diameter_est']/10,2)\n",
    "\n",
    "    sensors_filtered['vt_H20'] = np.where(sensors_filtered['d_H20']<15.9,\n",
    "                                                    np.power(7.6*sensors_filtered['d_H20']/10, 0.89),\n",
    "                                                    np.power(8.4*sensors_filtered['d_H20']/10, 0.67))\n",
    "\n",
    "    sensors_filtered['KE_H20'] = 0.5 * sensors_filtered['m_H20'] * sensors_filtered['vt_H20']**2\n",
    "\n",
    "    sensors_filtered['ratio_H20'] = sensors_filtered['d_H20']/sensors_filtered['diameter_est']\n",
    "\n",
    "    sensors_filtered = sensors_filtered.sort_values(['sensorName', 'Time'], ascending=[True, True])\n",
    "    sensors_filtered['Time2'] = pd.to_datetime(sensors_filtered['Time'])\n",
    "    sensors_filtered['Time2'] = sensors_filtered['Time2'].dt.tz_localize(timezone.utc)\n",
    "\n",
    "    conditions2 = [(sensors_filtered['kin_energy'] <= 0.07),\n",
    "                   ((sensors_filtered['kin_energy'] > 0.07) & (sensors_filtered['kin_energy'] <= 0.13)),\n",
    "                   ((sensors_filtered['kin_energy'] > 0.13) & (sensors_filtered['kin_energy'] <= 0.26)),\n",
    "                   ((sensors_filtered['kin_energy'] > 0.26) & (sensors_filtered['kin_energy'] <= 0.67)),\n",
    "                   ((sensors_filtered['kin_energy'] > 0.67) & (sensors_filtered['kin_energy'] <= 3.8)),\n",
    "                  (sensors_filtered['kin_energy'] > 3.8)]\n",
    "\n",
    "    val2 = [0.064, 0.18, 0.332, 0.5, 0.87, 0.994]\n",
    "    sensors_filtered['dead_time'] = np.select(conditions2, val2)\n",
    "    \n",
    "    return sensors_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e2e70d-89f0-4027-a43b-103746d39c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sensors_data(sensors_data, EVENT_MIN_TIME):\n",
    "    \n",
    "    data = sensors_filtered.copy()\n",
    "    data['Time_diff'] = data['Time2'].diff().dt.total_seconds()\n",
    "    data['Time_diff_p'] = np.where((data['Time_diff']<0) | (data['Time_diff'].abs()/EVENT_MIN_TIME >=1),1,0)\n",
    "    data = data.assign(Event_ID=data.Time_diff_p.cumsum())\n",
    "    data['Event_start'] = data.groupby(['Event_ID'])['Time2'].transform('min')\n",
    "    data['Event_end'] = data.groupby(['Event_ID'])['Time2'].transform('max')\n",
    "    data['Event_duration'] = (data['Event_end'] - data['Event_start']).dt.total_seconds()\n",
    "    data['Time_0'] = (data['Time2'] - data['Event_start']).dt.total_seconds()\n",
    "    data['Time_rel'] = data['Time_0']/data['Event_duration']\n",
    "    temp = data.groupby(['Event_ID'])\n",
    "    data = data.set_index(['Event_ID'])\n",
    "    data['Event_hits'] = temp.size()\n",
    "    data['Event_density'] = data['Event_hits']/SENSOR_SURFACE\n",
    "    data = data.reset_index()\n",
    "    data.drop(['Time_diff_p'], axis=1, inplace=True)\n",
    "    data['Event_tot_dead_time'] = data.groupby(['Event_ID'])['dead_time'].transform('sum')\n",
    "\n",
    "    data['Event_hit_rate'] = np.where(data['Event_duration']>0,\n",
    "                                                        data['Event_hits']/data['Event_duration'],\n",
    "                                                        0)\n",
    "\n",
    "    data['Event_hit_rate_adj2'] = np.where(data['Event_duration']>0,\n",
    "                                                        data['Event_hits']/(data['Event_duration'] - data['Event_tot_dead_time']),\n",
    "                                                        0)\n",
    "    data['Event_hits_adj2'] = np.where(data['Event_duration']>0,(data['Event_hit_rate_adj2'] * data['Event_duration']).round(0).astype(int),\n",
    "                                                    1)\n",
    "    data['Event_hits_diff2'] = data['Event_hits_adj2'] - data['Event_hits']\n",
    "\n",
    "    data['dt'] = data['Time_diff'].abs()\n",
    "    data['dt_next'] = data['Time2'].diff(periods=-1).dt.total_seconds().abs()\n",
    "    data['dt'] = data['dt'].where(data['dt']<=EVENT_MIN_TIME, other=np.NaN)\n",
    "    data['dt_next'] = data['dt_next'].where(data['dt']<=EVENT_MIN_TIME, other=np.NaN)\n",
    "    data['Event_dt_avg'] = data.groupby(['Event_ID'])['dt'].transform(lambda x: x.mean(skipna=True))\n",
    "    data['Event_dt_min'] = data.groupby(['Event_ID'])['dt'].transform(lambda x: x.min(skipna=True))\n",
    "    data['Event_inst_hit_rate'] = 1/data['dt']\n",
    "    data['Event_inst_hit_rate_max'] = data.groupby(['Event_ID'])['Event_inst_hit_rate'].transform(lambda x: x.max(skipna=True))\n",
    "    data['Event_inst_hit_rate_avg'] = data.groupby(['Event_ID'])['Event_inst_hit_rate'].transform(lambda x: x.mean(skipna=True))\n",
    "    data['Event_inst_hit_rate_std'] = data.groupby(['Event_ID'])['Event_inst_hit_rate'].transform(lambda x: x.std(skipna=True))\n",
    "    data['dt'].fillna(0, inplace=True)\n",
    "    data['dt_next'].fillna(0, inplace=True)\n",
    "    data['Event_inst_hit_rate'].fillna(0, inplace=True)\n",
    "\n",
    "    data['Occurrences'] = 1\n",
    "    roll_calc = data.groupby('Event_ID').rolling('10s', on='Time2', center=True)['Occurrences'].sum()\n",
    "    roll_calc2 = data.groupby('Event_ID').rolling('30s', on='Time2', center=True)['Occurrences'].sum()\n",
    "    data = data.set_index([\"Event_ID\", \"Time2\"])\n",
    "    data[\"10s_hit_rate\"] = roll_calc\n",
    "    data[\"30s_hit_rate\"] = roll_calc2\n",
    "    data=data.reset_index()\n",
    "    data['Event_10s_hit_rate_max'] = data.groupby(['Event_ID'])['10s_hit_rate'].transform('max')\n",
    "    data['Event_30s_hit_rate_max'] = data.groupby(['Event_ID'])['30s_hit_rate'].transform('max')\n",
    "    data['Event_10s_hit_rate_avg'] = data.groupby(['Event_ID'])['10s_hit_rate'].transform('mean')\n",
    "    data['Event_30s_hit_rate_avg'] = data.groupby(['Event_ID'])['30s_hit_rate'].transform('mean')\n",
    "    data['Event_10s_hit_rate_std'] = data.groupby(['Event_ID'])['10s_hit_rate'].transform('std')\n",
    "    data['Event_30s_hit_rate_std'] = data.groupby(['Event_ID'])['30s_hit_rate'].transform('std')\n",
    "    \n",
    "    conditions = [(data['Event_hits'] == 1),\n",
    "                   ((data['Event_hits'] > 1) & (data['Event_hits'] <= 5)),\n",
    "                   ((data['Event_hits'] > 5) & (data['Event_hits'] <= 25)),\n",
    "                  (data['Event_hits'] > 25)]\n",
    "\n",
    "    val = ['1 impact', '2-5 impacts', '6-25 impacts', '> 25 impacts']\n",
    "    data['Event_hits_group'] = np.select(conditions, val)\n",
    "\n",
    "    \n",
    "    data['Event_KEdensity_tot'] = data.groupby(['Event_ID'])['kin_energy'].transform('sum')/SENSOR_SURFACE\n",
    "    data['Event_KEFlux_tot'] = data['Event_KEdensity_tot']/data['Event_duration']\n",
    "    data['Event_KE_max'] = data.groupby(['Event_ID'])['kin_energy'].transform('max')\n",
    "    data['Event_D_max'] = data.groupby(['Event_ID'])['diameter'].transform('max')\n",
    "    data['Event_D_max_vicinity'] = (data['Event_D_max'] - 5.06)/0.493\n",
    "    \n",
    "    data['Event_count'] = data.groupby(['Event_hits'])['Event_ID'].transform('nunique')\n",
    "    data['meanCZCrad10_ev'] = data.groupby(['Event_ID'])['maxczc_rad10'].transform('mean')\n",
    "    data['meanCZCrad4_ev'] = data.groupby(['Event_ID'])['maxczc_rad4'].transform('mean')\n",
    "    data['meanCZCpix_ev'] = data.groupby(['Event_ID'])['czc_sensorpix'].transform('mean')\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39d6761-36f3-4b36-970c-79c7c57f864d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_hail_events_full(data):\n",
    "    \n",
    "    hail_events = data.groupby('Event_ID')['Event_hits','Event_hits_group','Event_count','Event_hits_adj2','Event_duration','Event_D_max','daily_count_by_sensor',\n",
    "                                           'Event_dt_min', 'Event_dt_avg','Event_tot_dead_time','Event_inst_hit_rate_avg','Event_10s_hit_rate_avg','Event_30s_hit_rate_avg',\n",
    "                                           'Event_inst_hit_rate_std','Event_10s_hit_rate_std','Event_30s_hit_rate_std',\n",
    "                                           'Event_hit_rate','Event_inst_hit_rate_max','Event_10s_hit_rate_max','Event_30s_hit_rate_max',\n",
    "                                           'Event_hit_rate_adj2', 'Event_hits_diff2',\n",
    "                                           'Event_density','Event_start','Event_end',\n",
    "                                           'sensorName','installDate','days',\n",
    "                                           'meanCZCpix_ev','meanCZCrad4_ev','meanCZCrad10_ev',\n",
    "                                           'Event_KEdensity_tot','Event_KEFlux_tot',\n",
    "                                           'Event_KE_max','Event_D_max_vicinity','region'].agg('min')\n",
    "\n",
    "    idx = data.groupby(['Event_ID'])['diameter'].transform(max) == data['diameter']\n",
    "    max_d = data.loc[idx,['Event_ID','Time_0']].set_index('Event_ID')\n",
    "    max_d2 = max_d.groupby('Event_ID').agg('min')\n",
    "    max_d_ = data.loc[idx,['Event_ID','Time_rel']].set_index('Event_ID')\n",
    "    max_d_2 = max_d_.groupby('Event_ID').agg('min')\n",
    "    df3 = pd.merge(hail_events, max_d2, how=\"inner\", on = 'Event_ID')\n",
    "    hail_events_full = pd.merge(df3, max_d_2, how=\"inner\", on = 'Event_ID')\n",
    "    \n",
    "    return hail_events_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a680c5f7-70cd-4869-97cd-2c1273b6a378",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sensors_sel(data, data2, HIT_TH, HIT_TH_UP):\n",
    "    hail_events_sel = data.loc[(data['Event_hits'] >= HIT_TH) & (data['Event_hits'] <= HIT_TH_UP)]\n",
    "    sensors_ev_sel = data2.loc[(data2['Event_hits'] >= HIT_TH) & (data2['Event_hits'] <= HIT_TH_UP)]\n",
    "    \n",
    "    return hail_events_sel, sensors_ev_sel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a976ff3d-2975-48ec-988e-9add6dcbf1cc",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data and methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2af75b-5090-4783-9ed3-dbb022e2a725",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Figure 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd130dc3-62b6-4ae9-8c7d-0b9a3f444184",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# First, let's load SwissTopo Relief! This will be our map background\n",
    "# read relief and data\n",
    "da_relief = xr.open_rasterio('relief_georef_clipped_swiss.tif')\n",
    "\n",
    "# Compute the lon/lat coordinates with rasterio.warp.transform\n",
    "ny, nx = len(da_relief['y']), len(da_relief['x'])\n",
    "x, y = np.meshgrid(da_relief['x'], da_relief['y'])\n",
    "# Rasterio works with 1D arrays\n",
    "transformer = Transformer.from_crs(da_relief.crs, outProj)\n",
    "lat, lon = transformer.transform(x.flatten(), y.flatten())\n",
    "lon = np.asarray(lon).reshape((ny, nx))-0.01\n",
    "lat = np.asarray(lat).reshape((ny, nx))\n",
    "da_relief.coords['lon'] = (('y', 'x'), lon)\n",
    "da_relief.coords['lat'] = (('y', 'x'), lat)\n",
    "\n",
    "# get band\n",
    "da_relief = da_relief.isel(band=0, drop=True)\n",
    "da_relief = da_relief.where(da_relief > 1, drop=True)\n",
    "\n",
    "# let's load Lakes from Swisstopo\n",
    "import geopandas\n",
    "gdf_lakes = geopandas.read_file(\"swiss-lakes-maps.json\")\n",
    "\n",
    "# cities and urban areas\n",
    "import cartopy.io.shapereader as shapereader\n",
    "\n",
    "cities = shapereader.natural_earth(resolution='10m',\n",
    "                                      category='cultural',\n",
    "                                      name='populated_places')\n",
    "gdf_cities = geopandas.read_file(cities)\n",
    "swiss_cities = gdf_cities.loc[gdf_cities['NAMEASCII'].isin(['Geneva','Zurich','Lausanne',\n",
    "                                      'Basel','Bern','Lugano',\n",
    "                                      'Luzern','Winterthur'])]\n",
    "\n",
    "gdf_urban2 = geopandas.read_file('data/ne_10m_urban_areas_landscan.shp')\n",
    "\n",
    "cmap = ListedColormap([\"tab:orange\", \"tab:green\", \"tab:red\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f690fd60-3047-493b-be73-a1e630c2dab9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sensor locations\n",
    "sns.set(style='ticks', context='paper')\n",
    "cm = 1/2.54  # centimeters in inches\n",
    "fig = plt.figure(figsize=(11, 10), dpi=DPI)\n",
    "prj = ccrs.AlbersEqualArea(8.222665776, 46.800663464)\n",
    "prj2 = ccrs.PlateCarree()\n",
    "ax = plt.axes(projection=prj)\n",
    "\n",
    "fs = 14\n",
    "sp = 100\n",
    "\n",
    "CH = [5.9, 10.7, 45.7, 47.9] #45.0\n",
    "ZRH = [7.9,9.3,46.95,47.7]\n",
    "Ticino2 = [8.83, 9, 46.00, 46.12]\n",
    "\n",
    "extent = CH\n",
    "ax.set_extent(extent)#,prj2)\n",
    "\n",
    "# Sensor data\n",
    "\n",
    "sensors = ax.scatter(sensors_list['Lon'],sensors_list['Lat'],\n",
    "            marker='o',\n",
    "            s=sp-20,\n",
    "            zorder=4, c=sensors_list['installDate'].dt.year,\n",
    "           edgecolors='black', linewidth=0.25, transform=ccrs.Geodetic(),cmap=cmap)\n",
    "\n",
    "handles, labels = sensors.legend_elements()\n",
    "labels = ['Spring 2018', 'Spring 2019', 'Spring 2020']\n",
    "legend1 = ax.legend(handles, labels, loc=\"upper left\", prop={'size': fs}, framealpha=1, markerscale=2, title='Time of installation')\n",
    "legend1.get_title().set_fontsize(fs)\n",
    "ax.add_artist(legend1)\n",
    "\n",
    "#add cities\n",
    "transform = prj2._as_mpl_transform(ax)\n",
    "for x, y, label in zip(swiss_cities.geometry.x, swiss_cities.geometry.y, swiss_cities.NAMEASCII):\n",
    "    ax.annotate(label, xy=(x, y), xycoords=transform, zorder=4,\n",
    "                fontsize=fs, xytext=(6, 4), textcoords=\"offset points\")\n",
    "\n",
    "#add sensor location labels\n",
    "ax.text(7.2,47.25,'Jura', zorder=5,\n",
    "        ha='center', va='center', size=fs+1, color ='black', weight='bold', transform=ccrs.PlateCarree())\n",
    "\n",
    "ax.text(8.15,46.8,'Napf', zorder=5,\n",
    "        ha='center', va='center', size=fs+1, color ='black', weight='bold', transform=ccrs.PlateCarree())\n",
    "\n",
    "ax.text(9.25,46.15,'Ticino', zorder=5,\n",
    "        ha='center', va='center', size=fs+1, color ='black', weight='bold', transform=ccrs.PlateCarree())\n",
    "\n",
    "#add urban areas\n",
    "gdf_urban2.geometry.plot(ax=ax,transform=prj2,zorder=1.4,linewidth=0,alpha=0.5, facecolor='red')\n",
    "\n",
    "def rect_from_bound(xmin, xmax, ymin, ymax):\n",
    "    \"\"\"Returns list of (x,y)'s for a rectangle\"\"\"\n",
    "    xs = [xmax, xmin, xmin, xmax, xmax]\n",
    "    ys = [ymax, ymax, ymin, ymin, ymax]\n",
    "    return [(x, y) for x, y in zip(xs, ys)]\n",
    "\n",
    "# request data for use by geopandas\n",
    "resolution = '10m'\n",
    "category = 'cultural'\n",
    "name = 'admin_0_countries'\n",
    "\n",
    "shpfilename = shapereader.natural_earth(resolution, category, name)\n",
    "df = geopandas.read_file(shpfilename)\n",
    "\n",
    "# get geometry of a country\n",
    "poly = [df.loc[df['ADMIN'] == 'Switzerland']['geometry'].values[0]]\n",
    "ax.add_geometries(poly, crs=prj2, facecolor='none', edgecolor='black', linewidth=0.2)\n",
    "\n",
    "pad1 = .1  #padding, degrees unit\n",
    "pad2 = .1 #.4\n",
    "exts = [poly[0].bounds[0] - pad1, poly[0].bounds[2] + pad1, poly[0].bounds[1] - pad2, poly[0].bounds[3] + pad1];\n",
    "ax.set_extent(exts, crs=prj2)\n",
    "\n",
    "# make a mask polygon by polygon's difference operation\n",
    "# base polygon is a rectangle, another polygon is simplified switzerland\n",
    "msk = Polygon(rect_from_bound(*exts)).difference(poly[0].simplify(0.01))\n",
    "msk_stm  = prj.project_geometry(msk, prj2)  # project geometry to the projection\n",
    "\n",
    "# plot the mask using semi-transparency (alpha=0.65) on the masked-out portion\n",
    "ax.add_geometries(msk_stm, prj, zorder=1.5, facecolor='white', edgecolor='none')\n",
    "    \n",
    "# add Relief\n",
    "da_relief.plot(ax=ax, x='lon', y='lat', cmap=\"Greys_r\",\n",
    "               norm=colors.Normalize(vmin=110, vmax=255), add_colorbar=False, transform=prj2)\n",
    "# add lakes\n",
    "gdf_lakes.plot(ax=ax, edgecolor='none', color=\"cornflowerblue\", transform=prj2)\n",
    "\n",
    "gl = ax.gridlines(crs=prj2, draw_labels=True, linewidth=0.25, color='gray', alpha=0.4, linestyle='--')\n",
    "gl.top_labels = False\n",
    "gl.right_labels = False\n",
    "gl.xlabel_style = {'size': fs}\n",
    "gl.ylabel_style = {'size': fs}\n",
    "gl.xformatter = LongitudeFormatter()\n",
    "gl.yformatter = LatitudeFormatter()\n",
    "\n",
    "ax.spines['geo'].set_linewidth(0.3)\n",
    "ax.spines['geo'].set_capstyle('butt')\n",
    "\n",
    "plt.savefig(foldername + 'sensor_map_3.png',bbox_inches = 'tight', pad_inches=0, format='png',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19092f78-38ef-4a3b-aa03-9eb7a561205a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap2 = ListedColormap([\"tab:orange\", \"tab:green\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb755191-49d6-410e-9df6-4a3c6e9158d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style='ticks', context='paper', font_scale=1.5)\n",
    "fig = plt.figure(figsize = (6,6), dpi = DPI)\n",
    "ax = plt.axes()\n",
    "sensors_list['x [km]'] = sensors_list['CHY']/1000\n",
    "sensors_list['y [km]'] = sensors_list['CHX']/1000\n",
    "data = sensors_list.loc[sensors_list['region']=='Jura'] #Ticino, Napf\n",
    "region = data['region'].iloc[0]\n",
    "sns.scatterplot(ax=ax,x=data['x [km]'], y=data['y [km]'], c=cmap([2]), s = 40,\n",
    "                edgecolor='black',linewidth=0.25)\n",
    "plt.grid(True)\n",
    "#ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "plt.savefig(foldername + 'sensor_location'+ '_' + region + '.png',bbox_inches = 'tight', dpi=DPI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4516f3c8-1f70-41f8-9fb3-e9f2d9789e8a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Figure 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdb2228-df7f-4a76-a176-005d49d88950",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_event_time_series(ID,min_time):\n",
    "    OUTPUT_FOLDER_TIME_SERIES = 'C:/.../day_sensor/'\n",
    "    plot_data = sensors_filtered[sensors_filtered['day_sensor_ID']==ID]\n",
    "    day = plot_data['days'].iloc[0]\n",
    "    sensor = plot_data['sensorName'].iloc[0]\n",
    "    region = plot_data['region'].iloc[0]\n",
    "    n_hits = plot_data['daily_count_by_sensor'].iloc[0]\n",
    "    fig = plt.figure(figsize = (13,4), dpi = DPI)\n",
    "    ax = plt.axes()\n",
    "    fs = 10\n",
    "    sns.set(style='ticks', context='paper',font_scale=1.8)\n",
    "    sns.scatterplot(x=plot_data['Time2'], y=plot_data['diameter'], color='red',s = 40, edgecolor='black', linewidth=0.4, ax=ax)\n",
    "\n",
    "    majorFmt = mdates.DateFormatter('%H:%M')\n",
    "    ax.xaxis.set_major_formatter(majorFmt)\n",
    "    plt.title('')\n",
    "    plt.suptitle('')\n",
    "    plt.ylabel('diameter [mm]')\n",
    "    plt.xlabel('Time UTC')\n",
    "    plt.ylim(0,30)\n",
    "    plt.yticks(np.arange(0, 35, step=5));\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.savefig(OUTPUT_FOLDER_TIME_SERIES + 'timeseries_' + day + '_' + region + '_' + sensor + '_' + str(n_hits) + '.png',bbox_inches = 'tight', dpi=DPI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8895be86-32eb-4cd1-a3c4-91c52bb4c222",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "day_list = sensors_filtered.loc[sensors_filtered['daily_count_by_sensor'] >= 5, 'day_sensor_ID'].unique()\n",
    "[plot_event_time_series(x,0) for x in day_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd861a9-cede-4bf8-859a-e7af5cc9d285",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_event_time_series(1,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b46900-e6a1-4aec-902f-b9f8289a72cc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# General observations, hailstones size and kinetic energy distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860aa48d-ab94-45be-b11b-17a5b22fc43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_filtered = make_sensors_filtered(reports_data, 'maxczc_rad4', 35)\n",
    "sensors_filtered = sensors_filtered.loc[sensors_filtered['diameter']>=5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d153b20-581a-4ec2-b4c5-2033581013e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sensors_filtered['diameter'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae96255-b149-4ab3-81b0-91a6accd31ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sensors_filtered['kin_energy'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d5f49e-38d1-4ee9-aa94-6ec8eeffd25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sensors_filtered[sensors_filtered['diameter']>=20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583eadef-a9d2-49db-b603-22047bb93b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "adj = 0.5\n",
    "min_D = 5\n",
    "D = np.arange(min_D, 35, 2*adj)\n",
    "n, d = np.histogram(sensors_filtered['diameter'], bins = D, density=False)\n",
    "n_norm = n/n.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea7db55-4ef4-408c-a4e3-f9edb0ddec42",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Figure 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3810ef7e-1fd2-4da0-9573-81b5ceb2d0b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sensors data - Nb of measurements by month\n",
    "\n",
    "nmeas_valid = []\n",
    "\n",
    "startdate = '2018-07'\n",
    "enddate = '2022-09'\n",
    "months = pd.date_range(startdate,enddate,freq='M').strftime('%Y-%m')\n",
    "\n",
    "for i,s in enumerate(months):\n",
    "    nmeas_valid.append(np.sum(sensors_filtered[sensors_filtered['months'] == s]['status'] == 0))\n",
    "                         \n",
    "plt.figure(figsize = (10,4), dpi = DPI)\n",
    "ax = plt.subplot()\n",
    "plt.bar(range(len(months)),nmeas_valid)\n",
    "plt.title('Number of sensor impacts per month')\n",
    "plt.xticks(range(len(months)), months, rotation = 90, fontsize = 12)\n",
    "min_loc = 1\n",
    "max_loc = 2\n",
    "ax.xaxis.set_major_locator(MultipleLocator(max_loc))\n",
    "ax.xaxis.set_minor_locator(MultipleLocator(min_loc))\n",
    "plt.margins(x=0.01, tight=True)\n",
    "plt.savefig(foldername + 'sensors_barplot_months.png',bbox_inches = 'tight',dpi=DPI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35b1e13-ac6a-4b8c-b6b6-4c5ca884bcd1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Exponential fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b1fab7-63f9-440c-9386-fc8479736f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def powerlaw(x, a, b, c):\n",
    "    return a*np.power(x,b) + c\n",
    "\n",
    "def powerlaw_2(x, a, b):\n",
    "    return a*np.power(x,b)\n",
    "\n",
    "def powerlaw_3(x, a, b):\n",
    "    return a * np.exp(-b * x)\n",
    "\n",
    "def powerlaw_4(x, a, b):\n",
    "    return a * np.power(10, -b * x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc5221a-8b92-4ec3-a933-cd15515989bb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Figure 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5a1ff4-7d9a-40b9-bff0-fa1012b48c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_diam_dist_prob(data):\n",
    "    sns.set(style='ticks', context='paper', font_scale=2)\n",
    "    plt.figure(figsize = (10,5), dpi=300)\n",
    "    ax = plt.axes()\n",
    "    #sns.histplot(x = data['diameter'], kde=False, stat='count',bins=30, cumulative=False, ax=ax, color='blue')\n",
    "    sns.histplot(x = data['diameter'], kde=False, stat='probability', binrange=(5,35), binwidth=1, cumulative=False, ax=ax, color='blue')\n",
    "    ax.scatter(d[0:-1] + adj,n_norm,color='tab:red')\n",
    "    popt, pcov = curve_fit(powerlaw_4, d[0:-1] + adj, n_norm)\n",
    "    err_pcov = np.sqrt(np.diag(pcov))\n",
    "    x_val = np.linspace(0,35,100)\n",
    "    pw1 = np.append(popt,err_pcov)\n",
    "    ax.plot(x_val, powerlaw_3(x_val, *popt), '-r', label='Exponential fit (eq. 3)')\n",
    "    #ax.set_xscale('log')\n",
    "    ax.set_yscale('log')\n",
    "    plt.xlim(4,36)\n",
    "    #plt.ylabel('# of impacts')\n",
    "    plt.xlabel('Diameter [mm]')\n",
    "    ax.legend()\n",
    "    plt.savefig(folder_dt_analysis  + 'Diameter_prob_distribution_minD5.png', bbox_inches = 'tight', dpi=DPI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9efd0a0-aeec-4f5f-a3c2-a3b22e637da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_diam_dist_prob(sensors_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f9f810-d75c-4317-b3ed-6f3045a10ed0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "popt, pcov = curve_fit(powerlaw_4, d[0:-1] + adj, n_norm)\n",
    "err_pcov = np.sqrt(np.diag(pcov))\n",
    "pw1 = np.append(popt,err_pcov)\n",
    "pw1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f9a4b7-f3c3-4b42-8de4-a16b4fb75208",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_kin_energy_dist(data):\n",
    "    sns.set(style='ticks', context='paper', font_scale=2)\n",
    "    plt.figure(figsize = (10,5), dpi=300)\n",
    "    ax = plt.axes()\n",
    "    sns.histplot(x = data['kin_energy'], kde=False, stat='count',bins=30,log_scale=True,cumulative=False, ax=ax, color='blue')\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_yscale('log')\n",
    "    plt.ylabel('# of impacts')\n",
    "    plt.xlabel('Kinetic energy [J]')\n",
    "    plt.savefig(folder_dt_analysis  + 'KE_distribution_' + '_CZC35_minD5.png', bbox_inches = 'tight', dpi=DPI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9ea76a-3c90-4101-8c10-b58624770e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_kin_energy_dist(sensors_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734b2487-6d01-483e-8b8b-8aa0842d47c8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Comparison with hailpads data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2168473a-fcab-4fa9-9e90-7ecc861d410d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hailpads_path = 'C:../Hail Pads/'\n",
    "hailpads_dist = 'manzato_hailpads_dist.csv'\n",
    "hailstones_dist = 'manzato_hailstones_dist.csv'\n",
    "hailpads_qt = 'hailpads_quantiles.csv'\n",
    "f_hailpad = 'hailpads_comp/'\n",
    "\n",
    "HAILPAD_MEAN = 8.030818\n",
    "HAILPAD_SD = 2.203626\n",
    "HAILPAD_N = 747757\n",
    "HAILPAD_SURFACE = 0.115\n",
    "\n",
    "hailpads_data = pd.read_csv(hailpads_path + hailpads_dist,sep=';')\n",
    "hailpads_data = hailpads_data.rename(columns={'stones_hailpad':'Daily_hits_per_device'})\n",
    "hailpads_data['Counts_norm'] = hailpads_data['Counts']/sum(hailpads_data['Counts'])\n",
    "hailpads_data['hits/m2'] = hailpads_data['Daily_hits_per_device']/HAILPAD_SURFACE\n",
    "hailpads_data['Device'] = 'Hailpads'\n",
    "\n",
    "plot_data = pd.concat([hailpads_data,hail_days])\n",
    "\n",
    "hailstones_data = pd.read_csv(hailpads_path + hailstones_dist,sep=';')\n",
    "hailstones_data = hailstones_data.rename(columns={'HailSize_BinCenter':'Size bin center'})\n",
    "hailstones_data = hailstones_data.rename(columns={'HailSize_Prob':'Probability'})\n",
    "hailstones_data['Size bin center adj'] = hailstones_data['Size bin center'] + 0.05\n",
    "\n",
    "qt_data = pd.read_csv(hailpads_path + hailpads_qt,sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238ef0e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "min_D = [5.95] #5.95\n",
    "adj = 0.25 # 0.25\n",
    "\n",
    "dens = False\n",
    "field_th = [35]\n",
    "field_name = ['maxczc_rad4']\n",
    "temp = []\n",
    "diam_dict = {}\n",
    "diam_dist_dict = {}\n",
    "D_list = []\n",
    "\n",
    "for i,j in zip(field_th,field_name):\n",
    "    for k in min_D:\n",
    "        sensors_data = make_sensors_filtered(reports_data, j, i)\n",
    "        sensors_data = sensors_data.loc[sensors_data['diameter']>=k]\n",
    "\n",
    "        diam = sensors_data['diameter']\n",
    "        D = np.arange(k, 47.45, 2*adj)\n",
    "        n, x = np.histogram(diam, bins = D, density=dens)\n",
    "        n = n/n.sum()\n",
    "        D_list.append(D)\n",
    "\n",
    "        df = sensors_data.groupby('day_sensor_ID')['daily_count_by_sensor'].agg('min').to_frame()\n",
    "        df = df.groupby('daily_count_by_sensor')['daily_count_by_sensor'].count().to_frame()\n",
    "        df = df.rename(columns={\"daily_count_by_sensor\":\"Counts\"}).reset_index()\n",
    "        df = df.rename(columns={\"daily_count_by_sensor\":\"Daily_hits_per_device\"})\n",
    "        df['Counts_norm'] = df['Counts']/sum(df['Counts'])\n",
    "        df['hits/m2'] = df['Daily_hits_per_device']/SENSOR_SURFACE\n",
    "        #df['device'] = 'hail sensor ' + j[3:6] + str(i) + '_' + str(k) + 'mm'\n",
    "        df['Device'] = 'Hail sensors'\n",
    "        temp.append(df)\n",
    "        diam_dist_dict[df['Device'].iloc[0]] = n\n",
    "        diam_dict[df['Device'].iloc[0]] = diam\n",
    "\n",
    "hail_days = pd.concat(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f434c8d-2824-4885-893f-a19533570179",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Figure 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae29b41-56ed-468e-8944-57aa4c353c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data['Device'] = pd.Categorical(plot_data['Device'], ['Hail sensors','Hailpads'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bc50e8-971c-451b-9073-46c09b89e6bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_devices_dist(data):\n",
    "    sns.set(style='ticks', context='paper', font_scale=2)\n",
    "    plt.figure(figsize = (10,5), dpi=300)\n",
    "    ax = plt.axes()\n",
    "    sns.histplot(x = data['Daily_hits_per_device'], weights = data['Counts'], kde=False, stat='probability', bins=50, cumulative=False, ax=ax, hue=data['Device'],multiple='dodge',common_norm=False)\n",
    "    ax.set_yscale('log')\n",
    "    plt.ylabel('Probability')\n",
    "    plt.xlabel('Number of hailstones per device')\n",
    "    plt.savefig(foldername + f_hailpad + 'Devices_distribution.png', bbox_inches = 'tight', dpi=DPI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8a369c-1a59-4d5d-b0d3-41abafa5f6bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_devices_dist(plot_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf755bab-c5a2-4d2d-904d-11cf15eb1d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_devices_dist_zoom(data):\n",
    "    sns.set(style='ticks', context='paper', font_scale=2)\n",
    "    plt.figure(figsize = (10,5), dpi=300)\n",
    "    ax = plt.axes()\n",
    "    sns.histplot(x = data['Daily_hits_per_device'], weights = data['Counts'], kde=False, stat='probability', bins=1245, cumulative=False, ax=ax, hue=data['Device'],multiple='dodge',common_norm=False)\n",
    "    #ax.set_yscale('log')\n",
    "    plt.ylabel('Probability')\n",
    "    plt.xlabel('Number of hailstones per device')\n",
    "    plt.xlim(0,50)\n",
    "    plt.savefig(foldername + f_hailpad + 'Devices_distribution_zoom.png', bbox_inches = 'tight', dpi=DPI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf157322-213e-46cc-a7f6-c1408306c0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_devices_dist_zoom(plot_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbb69c4-330b-462a-8966-9b1ce048019c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Figure 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7d8ec4-2d8f-448c-888f-02c0ae01f9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "popt, pcov = curve_fit(powerlaw_4,hailstones_data['Size bin center'].to_numpy(), hailstones_data['Probability'].to_numpy())\n",
    "err_pcov = np.sqrt(np.diag(pcov))\n",
    "pw = np.append(popt,err_pcov)\n",
    "popt2, pcov2 = curve_fit(powerlaw_4, x[0:-1] + adj, n)\n",
    "err_pcov2 = np.sqrt(np.diag(pcov2))\n",
    "pw2 = np.append(popt2,err_pcov2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c096e1-ea8b-4442-b4ed-91f0794108aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pw2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd472e1f-1b91-4294-a47a-10af76dbd2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "pw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0565371-5089-4595-a17d-0a89b948a085",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.set(style='ticks', context='paper', font_scale=1.5)\n",
    "plt.figure(figsize = (10,5), dpi=300)\n",
    "ax = plt.axes()\n",
    "\n",
    "for (key, value), d in zip(diam_dist_dict.items(), D_list):\n",
    "    ax.semilogy(d[0:-1] + adj, value,lw=2, label=key)\n",
    "    \n",
    "ax.semilogy(hailstones_data['Size bin center'].to_numpy(), hailstones_data['Probability'].to_numpy(),lw=2,label=\"Hailpads\")\n",
    "\n",
    "x_val = np.linspace(5.95,47.45,100)\n",
    "#\n",
    "\n",
    "ax.plot(x_val, powerlaw_4(x_val, *popt2), 'r--', label='Fit hail sensors (eq. 4)')\n",
    "ax.plot(x_val, powerlaw_4(x_val, *popt), 'g--', label='Fit hailpads (eq. 5)')\n",
    "\n",
    "\n",
    "plt.ylabel('Probability')\n",
    "plt.xlabel('Diameter [mm]')\n",
    "plt.xticks(np.arange(0, 50, step=5));\n",
    "plt.legend()\n",
    "plt.savefig(foldername + f_hailpad + 'HSD_ground_hailpad_sensor.png', bbox_inches = 'tight', dpi=DPI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5073d156-c1e4-42a9-b12a-c56b1f5ca2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style='ticks', context='paper', font_scale=1.5)\n",
    "plt.figure(figsize = (10,5), dpi=300)\n",
    "ax = plt.axes()\n",
    "\n",
    "for (key, value), d in zip(diam_dist_dict.items(), D_list):\n",
    "    ax.plot(d[0:-1] + adj, value,lw=2, label=key)\n",
    "\n",
    "ax.plot(hailstones_data['Size bin center'].to_numpy(), hailstones_data['Probability'].to_numpy(),lw=2,label=\"Hailpads\")\n",
    "#x_val = np.linspace(5.95,47.45,100)\n",
    "#\n",
    "#ax.plot(x_val, powerlaw_4(x_val, *popt), '-r', label='Exponential fit (eq. 3)')\n",
    "#ax.plot(x_val, powerlaw_4(x_val, *popt2), '-g', label='Exponential fit (eq. 3)')\n",
    "\n",
    "plt.ylabel('Probability')\n",
    "plt.xlabel('Diameter [mm]')\n",
    "plt.xticks(np.arange(0, 25, step=2));\n",
    "plt.xlim(5,20)\n",
    "plt.legend()\n",
    "plt.savefig(foldername + f_hailpad + 'HSD_ground_hailpad_sensor_lin_zoom.png', bbox_inches = 'tight', dpi=DPI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb98c1a-b158-4584-96b3-f47894b84dca",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Statistical analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df330a2-3516-4e84-a218-52d437bb77af",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Figure 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33351214-f2d3-49ee-a67b-add767be42cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "qt_dict = {}\n",
    "for key, value in diam_dict.items():\n",
    "    t = np.percentile(value, np.arange(1,101,1))\n",
    "    qt_dict[key] = t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388386a3-78bf-46b9-96f4-ebe01920c77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qq_plot(data,data_ref):\n",
    "    sns.set(style='ticks', context='paper', font_scale=2)\n",
    "    fig = plt.figure(figsize = (8,8), dpi=300)\n",
    "    ax = plt.axes()\n",
    "    for key, value in data.items():\n",
    "        sns.scatterplot(x = value, y = data_ref['Dhail'], ax=ax, s=30, linewidth=0, label=key)\n",
    "    sns.lineplot(x=data_ref['Dhail'], y = data_ref['Dhail'], color='red', ax=ax);\n",
    "    plt.xlabel('Quantile of diameter, hail sensors')\n",
    "    plt.ylabel('Quantile of diameter, hailpads')\n",
    "    plt.xticks(np.arange(0, 50, step=5))\n",
    "    plt.yticks(np.arange(0, 50, step=5))\n",
    "    plt.xlim(5,47)\n",
    "    plt.ylim(5,47)\n",
    "    ax.get_legend().remove()\n",
    "    plt.savefig(foldername + f_hailpad + 'HSD_QQplot.png', bbox_inches = 'tight', dpi=DPI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e35329b-6b7d-4cbc-a515-c68d9bfaa2bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "qq_plot(qt_dict,qt_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd152ead-64b7-4496-bee6-7e47398ebf56",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Chi-squared test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38c242d-b13b-450b-b2dd-645e9686053c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_bins = pd.DataFrame()\n",
    "#df_bins['hailpads_obs'] = hailstones_data['Probability'] * HAILPAD_N\n",
    "#df_bins['hailpads_obs'] = df_bins['hailpads_obs'].round(0).astype(int)\n",
    "name_list = []\n",
    "bins = qt_data['Dhail'].to_numpy()#[9::10]\n",
    "\n",
    "bins = np.insert(bins, 0, 5.95)\n",
    "\n",
    "for key, value in diam_dict.items():\n",
    "    df_bins[key] = pd.cut(value, bins=bins).value_counts().values\n",
    "    name_list.append(key)\n",
    "\n",
    "df_bins['hailpads_obs'] = HAILPAD_N/100\n",
    "df_bins['hailpads_obs'] = df_bins['hailpads_obs'].round(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5165cdb2-7abf-496f-a25e-2f0b571f812b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.stats import chisquare\n",
    "data = df_bins.head(100).copy()\n",
    "for f in name_list:\n",
    "    key_exp = f + '_exp'\n",
    "    data[key_exp] = data['hailpads_obs'] / np.sum(data['hailpads_obs']) * np.sum(data[f])\n",
    "    stat, p_value = chisquare(data[f], data[key_exp])\n",
    "    print(f\"Chi-squared Test: statistic={stat:.4f}, p-value={p_value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552fe9a9-33e7-4cfb-8aa2-bf70670bf831",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Standardized Mean Difference (SMD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f88ecbd-0159-445a-bb9d-1a04a4d27346",
   "metadata": {},
   "outputs": [],
   "source": [
    "smd_list = []\n",
    "for key, value in diam_dict.items():\n",
    "    m = np.mean(value)\n",
    "    std = np.std(value)\n",
    "    smd = abs(HAILPAD_MEAN - m)/np.sqrt((HAILPAD_SD**2 + std**2)/2)\n",
    "    smd_list.append(smd)\n",
    "print(f\"SMD={smd_list[0]:.4e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0bf7f2-50f1-4d22-bb70-8f17ef4b5bc6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Kolmogorov-Smirnov test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b28a74a-ce3c-442d-a847-7c36dc41dcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kstwo\n",
    "for key, value in qt_dict.items():\n",
    "    sample = qt_data['Dhail'].to_frame()\n",
    "    sample['Group'] = 'hailpads'\n",
    "    df = pd.DataFrame(value)\n",
    "    df = df.rename(columns={0: \"Dhail\"})\n",
    "    df['Group'] = 'hailsensors'\n",
    "    df_all = pd.concat([sample,df])\n",
    "    d_c = df_all.loc[df_all.Group=='hailpads', 'Dhail'].values\n",
    "    d_t = df_all.loc[df_all.Group=='hailsensors', 'Dhail'].values\n",
    "    df_ks = pd.DataFrame()\n",
    "    df_ks['Dhail'] = np.sort(df_all['Dhail'].unique())\n",
    "    df_ks['F_control'] = df_ks['Dhail'].apply(lambda x: np.mean(d_c<=x))\n",
    "    df_ks['F_treatment'] = df_ks['Dhail'].apply(lambda x: np.mean(d_t<=x))\n",
    "    k = np.argmax( np.abs(df_ks['F_control'] - df_ks['F_treatment']))\n",
    "    ks_stat = np.abs(df_ks['F_treatment'][k] - df_ks['F_control'][k])\n",
    "    m, n = len(sample), len(df)\n",
    "    en = m * n / (m + n)\n",
    "    p_value = kstwo.sf(ks_stat, np.round(en))\n",
    "    print(f\" Kolmogorov-Smirnov Test: statistic={ks_stat:.4f}, p-value={p_value:.4e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0cf262-191c-4f14-8f33-64e9fab1347d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Analysis of time-related quantities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5ab7ae-665d-4099-9e1e-78c1ffa8c2fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EVENT_MIN_TIME_LIST = [300,600,900,1200]\n",
    "EVENT_MIN_TIME_NAME = ['5 min','10 min','15 min','20 min']\n",
    "\n",
    "HIT_TH = 2\n",
    "HIT_TH_UP = 1000\n",
    "temp_events = []\n",
    "temp_hits = []\n",
    "field_name = 'maxczc_rad4'\n",
    "field_th = 35\n",
    "min_D = 5\n",
    "\n",
    "sensors_filtered = make_sensors_filtered(reports_data, field_name, field_th)\n",
    "sensors_filtered = sensors_filtered.loc[sensors_filtered['diameter']>=min_D]\n",
    "for i,j in zip(EVENT_MIN_TIME_LIST,EVENT_MIN_TIME_NAME):\n",
    "    sensors_data = make_sensors_data(sensors_filtered, i)\n",
    "    hail_events_full = make_hail_events_full(sensors_data)\n",
    "    hail_events_sel, sensors_ev_sel = make_sensors_sel(hail_events_full, sensors_data, HIT_TH, HIT_TH_UP)\n",
    "    \n",
    "    hail_events_sel['EVENT_MIN_TIME'] = i\n",
    "    hail_events_sel['EVENT_MIN_TIME_NAME'] = j\n",
    "    sensors_ev_sel['EVENT_MIN_TIME'] = i\n",
    "    sensors_ev_sel['EVENT_MIN_TIME_NAME'] = j\n",
    "    temp_events.append(hail_events_sel)\n",
    "    temp_hits.append(sensors_ev_sel)\n",
    "\n",
    "df_events = pd.concat(temp_events)\n",
    "df_hits = pd.concat(temp_hits)\n",
    "temp = df_events.groupby(['EVENT_MIN_TIME','Event_hits_group'])['Event_hits']\n",
    "df_events = df_events.reset_index()\n",
    "df_events = df_events.set_index(['EVENT_MIN_TIME','Event_hits_group'])\n",
    "df_events['Event_hits_group_size'] = temp.size()\n",
    "df_events['Event_hits_group_sum'] = temp.sum()\n",
    "df_events['Event_duration_min'] = df_events['Event_duration']/60\n",
    "df_events = df_events.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45c29a3-6bc3-4bb6-9388-d2f843e243dd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Dead time analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf7b629-7e7b-4be7-9b7c-24fe3f848779",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_events.groupby('EVENT_MIN_TIME')['Event_hits_diff2'].sum()/df_events.groupby('EVENT_MIN_TIME')['Event_hits'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3745ad8e-1b12-45aa-8821-2a42682f317b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Data for Table 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50824783-513e-48cf-a896-2ecb56f1d80f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Event duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fe25fd-75d0-470e-9eb8-61fc53c6f440",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_events.groupby(['EVENT_MIN_TIME_NAME'])['Event_duration_min'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f278a9-c010-4552-bc6b-1980fd91a792",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_events.groupby(['EVENT_MIN_TIME_NAME','Event_hits_group'])['Event_duration_min'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e6d3e3-3b99-4229-b59e-10786e047faa",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Cumulative time distribution of impacts (CTDI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79989610-22b6-4314-9328-8a6562a84e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Time0 in seconds, convert in minutes\n",
    "df_hits.groupby(['EVENT_MIN_TIME_NAME'])['Time_0'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de410447-e013-4653-a5fe-39a5092e078c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Time0 in seconds, convert in minutes\n",
    "df_hits.groupby(['EVENT_MIN_TIME_NAME','Event_hits_group'])['Time_0'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cb857a-494c-408d-90ee-3a6d4878661e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Figure 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0199125e-2666-4fab-8097-fd9e1cce4995",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_median_labels(ax, fmt='.1f'):\n",
    "    lines = ax.get_lines()\n",
    "    boxes = [c for c in ax.get_children() if type(c).__name__ == 'PathPatch']\n",
    "    lines_per_box = int(len(lines) / len(boxes))\n",
    "    for median in lines[4:len(lines):lines_per_box]:\n",
    "        x, y = (data.mean() for data in median.get_data())\n",
    "        # choose value depending on horizontal or vertical plot orientation\n",
    "        value = x if (median.get_xdata()[1] - median.get_xdata()[0]) == 0 else y\n",
    "        text = ax.text(x, y, f'{value:{fmt}}', ha='center', va='center',\n",
    "                       fontweight='bold', color='white')\n",
    "        # create median-colored border around white text for contrast\n",
    "        text.set_path_effects([\n",
    "            path_effects.Stroke(linewidth=3, foreground=median.get_color()),\n",
    "            path_effects.Normal(),\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257c866e-3a56-43dc-a65b-f458f8db46e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.set(style='ticks', context='paper', font_scale=1.5)\n",
    "plt.figure(figsize = (10,7), dpi=200)\n",
    "#plt.figure()\n",
    "ax = plt.axes()\n",
    "\n",
    "g = sns.boxplot(data=df_events, x=\"EVENT_MIN_TIME_NAME\", y=\"Event_duration_min\", hue=\"Event_hits_group\",hue_order=['2-5 impacts','6-25 impacts','> 25 impacts'],showfliers = False,ax=ax,saturation=1,palette='Set1')\n",
    "add_median_labels(g)\n",
    "plt.ylabel('Event duration [min]')\n",
    "plt.xlabel('$t_{mb}$ [min]')\n",
    "new_title = 'Impacts per event'\n",
    "leg = g.axes.get_legend()\n",
    "leg.set_title(new_title)\n",
    "\n",
    "plt.savefig(folder_dt_analysis + 'Boxplots_event_duration_CZC35_minD5_minutes.png',bbox_inches = 'tight',dpi=DPI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8201f104-6db4-4c83-abf4-20d3dee3b194",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Figure 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0af0a5-04cf-4288-8f36-c8a1ca263649",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [sns.light_palette(sns.color_palette('Set1')[0],8)[1],sns.light_palette(sns.color_palette('Set1')[1],8)[1],sns.light_palette(sns.color_palette('Set1')[2],8)[1],\n",
    "          sns.light_palette(sns.color_palette('Set1')[0],8)[3],sns.light_palette(sns.color_palette('Set1')[1],8)[3],sns.light_palette(sns.color_palette('Set1')[2],8)[3],\n",
    "          sns.light_palette(sns.color_palette('Set1')[0],8)[5],sns.light_palette(sns.color_palette('Set1')[1],8)[5],sns.light_palette(sns.color_palette('Set1')[2],8)[5],\n",
    "          sns.light_palette(sns.color_palette('Set1')[0],8)[7],sns.light_palette(sns.color_palette('Set1')[1],8)[7],sns.light_palette(sns.color_palette('Set1')[2],8)[7]]\n",
    "customPalette = sns.set_palette(sns.color_palette(colors))\n",
    "\n",
    "colors_2 = [sns.color_palette('Greys',8)[1],sns.color_palette('Greys',8)[3],sns.color_palette('Greys',8)[5],sns.color_palette('Greys',8)[7]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1514538f-d57c-44bc-9dba-e35ae8bbe76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style='ticks', context='paper', font_scale=1.5)\n",
    "plt.figure(figsize = (10,7), dpi=200)\n",
    "#plt.figure()\n",
    "ax = plt.axes()\n",
    "\n",
    "hue_order = ['$t_{mb}$: 5 min, 2-5 impacts','$t_{mb}$: 5 min, 6-25 impacts','$t_{mb}$: 5 min, > 25 impacts',\n",
    "            '$t_{mb}$: 10 min, 2-5 impacts','$t_{mb}$: 10 min, 6-25 impacts','$t_{mb}$: 10 min, > 25 impacts',\n",
    "            '$t_{mb}$: 15 min, 2-5 impacts','$t_{mb}$: 15 min, 6-25 impacts','$t_{mb}$: 15 min, > 25 impacts',\n",
    "            '$t_{mb}$: 20 min, 2-5 impacts','$t_{mb}$: 20 min, 6-25 impacts','$t_{mb}$: 20 min, > 25 impacts']\n",
    "hue = '$t_{mb}$: ' + df_hits['EVENT_MIN_TIME_NAME'] + ', ' + df_hits['Event_hits_group'].astype(str)\n",
    "sns.ecdfplot(data=df_hits, x=\"Time_0\", hue=hue, hue_order=hue_order,palette = sns.color_palette(colors))\n",
    "\n",
    "plt.axhline(y=0.75, color='black', linestyle='-')\n",
    "\n",
    "plt.xlim(-10,700)\n",
    "plt.ylabel('Cumulative time distribution of impacts (CTDI)')\n",
    "plt.xlabel('Time since event start [s]')\n",
    "\n",
    "plt.savefig(folder_dt_analysis + 'Perc_hits_event_time_CZC35_minD5.png',bbox_inches = 'tight',dpi=DPI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2a3537-186a-4e4a-b576-13b3e5b9a344",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Figure 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938a9b8f-b39b-41ed-a9cd-75935f60d19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_event_hits_hitrate_density(data,EVENT_MIN_TIME, field):\n",
    "    data = data.sort_values([field], ascending=[True])\n",
    "    cmap = sns.color_palette(\"flare\", as_cmap=True)\n",
    "    sns.set(style='ticks', context='paper', font_scale=1.5)\n",
    "    plt.figure(figsize = (7,7), dpi=300)\n",
    "    ax = plt.axes()\n",
    "    ax.set(xscale=\"log\", yscale=\"log\")\n",
    "    g = sns.scatterplot(x=data['Event_hit_rate'], y=data['Event_hits'], hue=data[field],size=data[field],ax=ax,sizes=(10, 100), palette=cmap, s=40)\n",
    "    #sns.histplot(x = data['Event_hit_rate'], y = data['Event_hits'], kde=True, stat='count',cumulative=False, ax=ax, color='blue')\n",
    "    #sns.relplot(x=data['Event_hit_rate'], y = data['Event_hits'], hue=data['Event_30s_hit_rate_max'], size=data['Event_30s_hit_rate_max'],\n",
    "    #            sizes=(10, 100), palette=cmap, s=40)\n",
    "    plt.ylabel('Number of impacts per event')\n",
    "    plt.xlabel('Average hit rate [#/s]')\n",
    "    plt.xlim(0.002,2)\n",
    "    plt.ylim(1,405)\n",
    "    new_title = 'Maximum \\ninstanteanous \\nhit rate [#/s]'\n",
    "    leg = g.axes.get_legend()\n",
    "    leg.set_title(new_title)\n",
    "    # norm = plt.Normalize(data[field].min(), data[field].max())\n",
    "    # sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "    # sm.set_array([])\n",
    "    # ax.get_legend().remove()\n",
    "    # ax.figure.colorbar(sm)\n",
    "    \n",
    "    plt.savefig(folder_dt_analysis + 'Event_hits_hitrate_' + field + '_T' + str(EVENT_MIN_TIME) + '.png', bbox_inches = 'tight', dpi=DPI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b004a97b-2e4c-4904-a2c1-0dddbd462375",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_event_hits_hitrate_density(df_events[df_events['EVENT_MIN_TIME']==600],600,'Event_inst_hit_rate_max')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb048d2-1f04-4c71-bc83-fb90cb7ca969",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Figure 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2d5213-20a1-4892-8121-f1189a361e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style='ticks', context='paper', font_scale=1.5)\n",
    "plt.figure(figsize = (10,7), dpi=200)\n",
    "#plt.figure()\n",
    "ax = plt.axes()\n",
    "\n",
    "data = df_events.loc[df_events['Event_hits_group'].isin(['6-25 impacts','> 25 impacts'])]\n",
    "\n",
    "cm1 = [sns.color_palette('Set1',3)[1],sns.color_palette('Set1',3)[2]]\n",
    "\n",
    "g = sns.boxplot(data=data, x=\"EVENT_MIN_TIME_NAME\", y=\"Time_rel\", hue=\"Event_hits_group\",hue_order=['6-25 impacts','> 25 impacts'],\n",
    "                showfliers = False,ax=ax,saturation=1,palette=cm1)\n",
    "add_median_labels(g,'.2f')\n",
    "\n",
    "plt.ylabel('Relative time of maximum diameter [s]')\n",
    "plt.xlabel('$t_{mb}$ [s]')\n",
    "new_title = 'Impacts per event'\n",
    "leg = g.axes.get_legend()\n",
    "leg.set_title(new_title)\n",
    "\n",
    "plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)\n",
    "\n",
    "plt.savefig(folder_dt_analysis + 'Boxplots_DmaxReltime_CZC35_minD5.png',bbox_inches = 'tight',dpi=DPI)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
